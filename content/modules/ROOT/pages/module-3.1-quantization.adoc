:imagesdir: ../assets/images
= Weights and Activation Quantization (W4A16)

== Learning Objectives

By completing this exercise, you will:

* Understand W4A16 quantization and its benefits for LLM optimization
* Learn to set up a complete quantization pipeline using OpenShift AI
* Gain hands-on experience with SmoothQuant and GPTQ quantization techniques
* Evaluate quantized model performance and memory efficiency

== Overview

In this exercise, you will use a Jupyter notebook to investigate how LLM weights and activations can be quantized to **W4A16** format. This quantization method reduces memory usage while maintaining model performance during inference.

**W4A16 Quantization** compresses model weights to 4-bit precision while keeping activations at 16-bit precision, achieving significant memory savings with minimal accuracy loss.

**Key Quantization methods:**

* **SmoothQuant**: Reduces activation outliers by smoothing weight and activation quantization
* **GPTQ**: Post-training quantization method that maintains model quality through careful calibration

You'll follow these main steps to quantize your model:

1. **Load the model**: Load the pre-trained LLM model
2. **Choose the quantization scheme and method**: Refer to the slides for a quick recap of the schemes and formats supported
3. **Prepare calibration dataset**: Prepare the appropriate dataset for calibration
4. **Quantize the model**: Convert the model weights and activations to **W4A16** format
   * Using SmoothQuant and GPTQ
5. **Save the model**: Save the quantized model to suitable storage
6. **Evaluate the model**: Evaluate the quantized model's accuracy

== Prerequisites
Before beginning the quantization exercise, complete these setup steps:

* Create a Data Science Project
* Create Data Connections - To store the quantized model
* Deploy a Data Science Pipeline Server
* Launch a Workbench


=== Creating a Data Science Project
_Estimated time: 1-2 minutes_

First, create a project to organize your quantization work.

* Navigate to **Data Science Projects** in the left menu of the OpenShift AI Dashboard:
+
[.bordershadow]
image::quant-ds-proj-nav.png[title="Navigate to Data Science Projects in OpenShift AI Dashboard", link=self, window=blank, width=100%]

* Create a Data Science Project with the name `quantization`:
+
[.bordershadow]
image::quant-create-project.png[title="Create Data Science Project Named 'quantization'", link=self, window=blank, width=100%]

Verify that your project appears in the Data Science Projects list with the name "quantization" and shows a "Ready" status.

=== Creating a Data Connection for the Pipeline Server
_Estimated time: 2-3 minutes_

Now configure the data connection to link your pipeline server to MinIO storage.

* Navigate to the `quantization` project in the OpenShift AI dashboard
* Click **Add Data Connection**:

[.bordershadow]
image::quant-add-dc.png[title="Add Data Connection Button in quantization Project", link=self, window=blank, width=100%]


* Select the connection type **S3-compatible object storage -v1** and use the following values for configuring the MinIO connection:
+
[.bordershadow]
image::quant-add-dc-type.png[title="Select S3-compatible Object Storage Connection Type", link=self, window=blank, width=100%]

** Name:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
pipeline-connection
** Access Key:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
minio
** Secret Key:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
minio123
** Endpoint:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
http://minio-service.minio.svc.cluster.local:9000
** Region:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
none
** Bucket:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
pipelines

* Verify your connection configuration matches this example:
+
[.bordershadow]
image::quant-data-connection.png[title="Data Connection Configuration", link=self, window=blank, width=100%]

* Create a second Data Connection named `minio-models`:
  - Use the same MinIO connection details as above
  - Change the bucket name to `models`

Check that both data connections appear in your project:

* `pipeline-connection` - connected to `pipelines` bucket
* `minio-models` - connected to the `models` bucket.

=== Creating a Pipeline Server
_Estimated time: 5-8 minutes_

Create the pipeline server before setting up your workbench.

* In the **quantization** project, navigate to **Data science pipelines** > **Pipelines**
* Click **Configure Pipeline Server**:
+
[.bordershadow]
image::quant-pipelineserver01.png[title="Configure Pipeline Server Button Location", link=self, window=blank, width=100%]

* Select the **pipeline-connection** Data Connection you created earlier
* Click **Configure Pipeline Server**:
+
[.bordershadow]
image::quant-pipelineserver02.png[title="Pipeline Server Configuration with Data Connection", link=self, window=blank, width=100%]

* Wait a few minutes for the pipeline server to deploy. The Pipelines section will display this status:
+
[.bordershadow]
image::quant-pipelineserver03.png[title="Successfully Deployed Pipeline Server Status", link=self, window=blank, width=100%]

NOTE: This may take a few minutes to complete. There is no need to wait for the pipeline server to be ready. You may proceed to the next steps and check this later.

Verify that the pipeline server is running:
* No error messages appear in the pipelines section

=== Creating a Workbench
_Estimated time: 4-8 minutes (including startup)_

With your Data Connection and Pipeline Server configured, you can now create the workbench environment.

* In the `quantization` project, click **Create a workbench**:
+
[.bordershadow]
image::quant-create-wb.png[title="Create Workbench Button in quantization Project", link=self, window=blank, width=100%]
* Configure the workbench with these settings:
** **Name**: `granite-quantization` (or your preferred name)
** **Image Selection**: `CUDA`
** **Container Size**: `Standard`
** **Accelerator**: `NVIDIA-GPU`
** **Number of accelerators**: `1`
+
[.bordershadow]
image::quant-launch-workbench-01.png[title="Workbench Configuration Settings with GPU Accelerator", link=self, window=blank, width=100%]
* Attach the **minio-models** Data Connection:
  - Click the **Connections** section
  - Select **Attach existing connections**
  - Click **Attach** for the **minio-models** connection
+
[.bordershadow]
image::quant-add-dc-01.png[title="Select Attach Existing Connections Option", link=self, window=blank, width=100%]
+
[.bordershadow]
image::quant-attach-dc.png[title="Attach minio-models Data Connection to Workbench", link=self, window=blank, width=100%]

* Click **Create Workbench** and wait for it to start
* When the workbench status shows **Running**, click the link beside its name to open it:
+
[.bordershadow]
image::quant-open-link.png[title="Click Workbench Link to Launch Jupyter", link=self, window=blank, width=100%]

* Authenticate with your OpenShift login credentials
* You will be asked to accept the following settings:
+
[.bordershadow]
image::quant-accept.png[title="Accept Jupyter Notebook Server Settings", link=self, window=blank, width=100%]

* After accepting the settings, the Jupyter interface will load:
+
[.bordershadow]
image::quant-jupyter.png[title="Jupyter Interface Successfully Loaded", link=self, window=blank, width=100%]

Confirm that:

* Workbench status shows "Running" 
* Jupyter interface loads without errors
* You can see the file browser and available kernels in the workbench
* GPU is accessible (if applicable) from the workbench

=== Clone the repository
_Estimated time: 1-2 minutes_

With Jupyter running, clone the exercise repository to access the quantization notebooks.

* Open the Git UI in Jupyter:
+
[.bordershadow]
image::quant-git-clone-1.png[title="Open Git Clone Interface in Jupyter", link=self, window=blank, width=100%]

* Specify the Git repository as:
+
[source,sh,role=execute]
----
https://github.com/redhat-ai-services/etx-llm-optimization-and-inference-leveraging.git
----
+
[.bordershadow]
image::quant-git-clone-2.png[title="Successfully Cloned Repository in Jupyter Environment", link=self, window=blank, width=100%]

You have now completed the setup and can proceed with the quantization exercise.

Before starting the quantization exercise, verify:

* Repository is cloned successfully in Jupyter
* `/workshop_code/quantization/llm_compressor` folder exists and contains:
  - `weight_activation_quantization.ipynb` notebook
  - `minio.yaml` file
  - `quantization_pipeline.py` file
* Data connections are accessible from the workbench environment
* GPU resources are available for quantization tasks

== Exercise: Quantize the Model with llm-compressor
_Estimated time: 15-20 minutes (depending on model size and GPU performance)_

Now you'll perform the actual quantization using the provided Jupyter notebook.

* Navigate to the `workshop_code/quantization/llm_compressor` folder
* Open the notebook `weight_activation_quantization.ipynb`:

[.bordershadow]
image::quantization-int8-notebook.png[title="Weight Activation Quantization Notebook Ready to Execute", link=self, window=blank, width=100%]

To execute the cells, you can select them and either click on the **play** icon or press **Shift + Enter**:
[.bordershadow]
image::quantization-notebook-cell-status.png[title="How to Execute Notebook Cells Using Play Button", link=self, window=blank, width=100%]

When the cell is being executed, you can see **[*]**. Once the execution has completed, you will see a number instead of the *, e.g., **[1]**:
[.bordershadow]
image::quantization-notebook-execute-cell.png[title="Cell Execution Status Indicators in Notebook", link=self, window=blank, width=100%]

When you complete the notebook exercises, close the notebook and continue to the next module.

IMPORTANT: Once you complete this exercise and you no longer need the workbench, ensure you **stop it** so that the associated GPU gets freed and can be utilized to serve the model.
[.bordershadow]
image::quantization-notebook-workbench-done.png[title="Access Workbench Actions Menu", link=self, window=blank, width=100%]
[.bordershadow]
image::quantization-notebook-workbench-stop.png[title="Stop Workbench to Free GPU Resources", link=self, window=blank, width=100%]