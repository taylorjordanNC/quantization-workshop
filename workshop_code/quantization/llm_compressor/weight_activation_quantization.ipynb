{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7854d876-3cd1-4a18-bf9c-4c947166fd88",
   "metadata": {},
   "source": [
    "# Quantization of `granite-3.3-2b-instruct` model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b258eb-3223-4594-860e-daae9d3a5c1d",
   "metadata": {},
   "source": [
    "Recall that our overall solution uses the quantized version of the model `granite-3.3-2b-instruct`. In this lab, we will be taking in the base model `granite-3.3-2b-instruct` and quantizing it to `W4A16` - which is fixed-point integer (INT) quantization scheme for weights and floating‑point for activations - to provide both memory savings (weight - INT4) and inference acceleration (activations - BF16) with `vLLM`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800f926-dc4c-42a1-84a7-03be28d7463f",
   "metadata": {},
   "source": [
    "**Note**: `W4A16` computation is supported on Nvidia GPUs with compute capability > 7.5 (Turing, Ampere, Ada Lovelace, Hopper).\n",
    "\n",
    "**Note**: The steps here will take around 20-30 minutes, depending on the connectivity. The most time consuming steps are the installation of llmcompressor (up to 5 mins) and the quantization step (which can take more anywhere between 10-15 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef99081-3a8b-486c-9882-deb387c83850",
   "metadata": {},
   "source": [
    "## Setting up llm-compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53691745",
   "metadata": {},
   "source": [
    "Installing `llmcompressor` may take a minute, depending on the bandwith available. Do note the versions of `transformer` library we would be using. There is a known issue (*torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow*) with the usage of the latest transformer library (version `4.53.2` as of July 17, 2025) in combination with the latest version of llmcompressor (version `0.6.0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037b244-10f5-4aac-981a-3a07864e0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llmcompressor==0.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c484403-9fd6-489d-9912-8e7153c97749",
   "metadata": {},
   "source": [
    "Let's make sure we have installed the right versions installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9e9b1-1e04-4c1c-be37-617c489b2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep llmcompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405cb5c-843d-4f12-8fbc-fa1e8db4f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24bf14-c77b-4f07-851d-d70cc61646eb",
   "metadata": {},
   "source": [
    "## Let' start with the quantization of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55327b5b-b309-4c23-a245-70d78d0955fc",
   "metadata": {},
   "source": [
    "There are 6 steps:\n",
    "1. Loading the model\n",
    "2. Choosing the quantization scheme and method\n",
    "3. Preparing the calibration data\n",
    "4. Applying quantization\n",
    "5. Saving the model\n",
    "6. Evaluation of accuracy in vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad553f-1f6f-4583-b280-6205930debe8",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d3096-978c-415a-9529-45f8b455e577",
   "metadata": {},
   "source": [
    "First, let's download the model from local S3 and then load the model using AutoModelForCausalLM for handling quantized saving and loading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1df99-46b5-45a6-98f5-cea3adae88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from boto3 import client\n",
    "\n",
    "# def download_model_from_s3(s3_path, local_model_dir):\n",
    "#     print('Starting download of model from S3')\n",
    "    \n",
    "#     # Get S3 credentials from environment\n",
    "#     s3_endpoint_url = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "#     s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "#     s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "#     s3_bucket_name = os.environ[\"AWS_S3_BUCKET\"]\n",
    "\n",
    "#     print(f'Downloading model from bucket {s3_bucket_name} '\n",
    "#           f'path {s3_path} from S3 storage at {s3_endpoint_url}')\n",
    "#     print(f'Target local directory: {local_model_dir}')\n",
    "\n",
    "#     s3_client = client(\n",
    "#         's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,\n",
    "#         aws_secret_access_key=s3_secret_key, verify=False\n",
    "#     )\n",
    "\n",
    "#     os.makedirs(local_model_dir, exist_ok=True)\n",
    "\n",
    "#     print(f'Listing objects with prefix: {s3_path}')\n",
    "#     paginator = s3_client.get_paginator('list_objects_v2')\n",
    "#     pages = paginator.paginate(Bucket=s3_bucket_name, Prefix=s3_path)\n",
    "\n",
    "#     downloaded_files = []\n",
    "#     total_objects = 0\n",
    "    \n",
    "#     for page in pages:\n",
    "#         if 'Contents' in page:\n",
    "#             for obj in page['Contents']:\n",
    "#                 total_objects += 1\n",
    "#                 s3_key = obj['Key']\n",
    "#                 print(f'Found S3 object: {s3_key}')\n",
    "                \n",
    "#                 # Skip if it's just a directory marker\n",
    "#                 if s3_key.endswith('/'):\n",
    "#                     print(f'Skipping directory marker: {s3_key}')\n",
    "#                     continue\n",
    "                \n",
    "#                 relative_path = s3_key[len(s3_path):].lstrip('/')\n",
    "#                 local_file_path = os.path.join(local_model_dir, relative_path)\n",
    "                \n",
    "#                 print(f'S3 key: {s3_key}')\n",
    "#                 print(f'Local file path: {local_file_path}')\n",
    "                \n",
    "#                 local_dir = os.path.dirname(local_file_path)\n",
    "#                 if local_dir:\n",
    "#                     os.makedirs(local_dir, exist_ok=True)\n",
    "#                     print(f'Created directory: {local_dir}')\n",
    "                \n",
    "#                 try:\n",
    "#                     s3_client.download_file(s3_bucket_name, s3_key, local_file_path)\n",
    "#                     downloaded_files.append(local_file_path)\n",
    "#                     print(f'Successfully downloaded {s3_key} to {local_file_path}')\n",
    "#                 except Exception as e:\n",
    "#                     print(f'Error downloading {s3_key}: {e}')\n",
    "#                     raise\n",
    "\n",
    "#     print(f'Total objects found: {total_objects}')\n",
    "#     print(f'Files downloaded: {len(downloaded_files)}')\n",
    "    \n",
    "#     config_path = os.path.join(local_model_dir, 'config.json')\n",
    "#     if os.path.exists(config_path):\n",
    "#         print(f'Verified: config.json found at {config_path}')\n",
    "#     else:\n",
    "#         print(f'Warning: config.json not found at {config_path}')\n",
    "    \n",
    "#     print('Finished downloading model from S3')\n",
    "#     return local_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141b43a-5483-41e6-b33d-34ceb891e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_dir = os.getcwd()\n",
    "# BASE_MODEL_DIR = current_dir + \"/granite-3.3-2b-instruct\"\n",
    "# S3_MODEL_PATH = \"ibm-granite/granite-3.3-2b-instruct/\"  \n",
    "\n",
    "# model_path = download_model_from_s3(S3_MODEL_PATH, BASE_MODEL_DIR)\n",
    "\n",
    "# # use the downloaded model path for tokenizer and model loading\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_path, device_map=\"auto\", torch_dtype=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99299f19",
   "metadata": {},
   "source": [
    "Alternatively, the model can also be loaded from HuggingFace directly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad8b7b-bb67-4d02-8cce-f80de7462f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"ibm-granite/granite-3.2-2b-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9742346d-f226-4462-871a-bf4008a04091",
   "metadata": {},
   "source": [
    "### Prepare calibration data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bdaadd-11b8-4627-be3d-3a1ffa4c9f88",
   "metadata": {},
   "source": [
    "Prepare the calibration data. When quantizing weigths of a model to int4 using GPTQ, we need some sample data to run the GPTQ algorithms. As a result, it is very important to use calibration data that closely matches the type of data used in our deployment. If you have fine-tuned a model, using a sample of your training data is a good idea.\n",
    "\n",
    "In our case, we are quantizing an Instruction tuned generic model, so we will use the ultrachat dataset. Some best practices include:\n",
    "- 512 samples is a good place to start (increase if accuracy drops). We are going to use 256 to speed up the process.\n",
    "- 2048 sequence length is a good place to start\n",
    "- Use the chat template or instrucion template that the model is trained with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3b1c6-f408-4ec5-81a0-3e8ae8019070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "NUM_CALIBRATION_SAMPLES = 512  # 1024\n",
    "DATASET_ID = \"neuralmagic/LLM_compression_calibration\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Load dataset.\n",
    "ds = load_dataset(DATASET_ID, split=DATASET_SPLIT)\n",
    "ds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n",
    "\n",
    "# Preprocess the data into the format the model is trained with.\n",
    "def preprocess(example):\n",
    "    return {\"text\": example[\"text\"]}\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c889b-b827-4f8d-be34-2cc05bba48f1",
   "metadata": {},
   "source": [
    "With the dataset ready, we will now apply quantization.\n",
    "\n",
    "We first select the quantization algorithm. For W4A16, we want to:\n",
    "- Run SmoothQuant to make the activations easier to quantize\n",
    "- Quantize the weights to 4 bits with channelwise scales using GPTQ\n",
    "- Quantize the activations with dynamic per token strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1732929",
   "metadata": {},
   "source": [
    "**Note**: The quantization step takes a long time to complete due to the callibration requirements -- around 10 - 15 mins, depending on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593615d-db1d-4b80-bd94-93868a45cd01",
   "metadata": {},
   "source": [
    "### Imports and definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da32a22-49bd-4480-b0e0-b0e052190ce0",
   "metadata": {},
   "source": [
    "**GPTQModifier**: Applies Gentle Quantization (GPTQ) for weight-only quantization.\n",
    "\n",
    "**SmoothQuantModifier**: Prepares model activations for smoother quantization by scaling internal activations and weights.\n",
    "\n",
    "**oneshot**: High-level API that applies your quantization recipe in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c06b7f-aedb-4e06-98cd-16aed3abd046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor.transformers import oneshot\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556fec70-e27b-4bab-8e6b-f6268636c5d9",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12866bd2-e08a-419c-98c7-ced3e5853baa",
   "metadata": {},
   "source": [
    "Rationale\n",
    "- **DAMPENING_FRAC=0.1** gently prevents large Hessian-derived updates during quantization.\n",
    "- **OBSERVER=\"mse\"** measures quantization error by squared deviations, yielding well-rounded scales.\n",
    "- **GROUP_SIZE=128** determines group size for per-channel quantization; typical default usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cf573-dc89-4b5d-9414-aad06992c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAMPENING_FRAC = 0.1  # tapering adjustment to prevent extreme weight updates\n",
    "OBSERVER = \"mse\"  # denotes minmax - quantization layout based on mean‐squared‐error\n",
    "GROUP_SIZE = 128  # # per-channel grouping width for quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb39b8-9fe3-4a7e-adba-9a03ec033344",
   "metadata": {},
   "source": [
    "### Layer Mappings & Ignoring Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d76f3-61a5-474f-8b83-3005199b7122",
   "metadata": {},
   "source": [
    "Logic\n",
    "\n",
    "- **ignore=[\"lm_head\"]** skips quantization on the output layer to preserve final logits and maintain accuracy.\n",
    "- mappings link groups of linear projections (q, k, v, gating, up/down projections) with layernorm blocks—SmoothQuant uses these to shift and normalize activations across paired layers for better quant distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4604b8-b311-428f-838e-a50bfc041b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore=[\"lm_head\"]\n",
    "mappings=[\n",
    "    [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*input_layernorm\"],\n",
    "    [[\"re:.*gate_proj\", \"re:.*up_proj\"], \"re:.*post_attention_layernorm\"],\n",
    "    [[\"re:.*down_proj\"], \"re:.*up_proj\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8918c2e-d6f1-4e31-aa56-57b5a7c3f1df",
   "metadata": {},
   "source": [
    "### Recipe Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291ad00-d881-4efb-bba5-2b20d3b28a3d",
   "metadata": {},
   "source": [
    "**Workflow**\n",
    "\n",
    "- **SmoothQuantModifier**: Re-scales activations across paired layers before quantization to reduce outliers (smoothing_strength=0.7, high smoothing but not extreme).\n",
    "- **GPTQModifier**: Performs Weight-Only quantization (4-bit weights, 16-bit activations) on all Linear layers except those ignored, applying your dampening and observer settings. Scheme \"W4A16\" reduces model size while maintaining decent accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abec4d4-a2a8-4e98-a813-b24e0aa8dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe = [\n",
    "#     SmoothQuantModifier(smoothing_strength=0.7, ignore=ignore, mappings=mappings),\n",
    "#     GPTQModifier(\n",
    "#         targets=[\"Linear\"],\n",
    "#         ignore=ignore,\n",
    "#         scheme=\"W4A16\",\n",
    "#         dampening_frac=DAMPENING_FRAC,\n",
    "#         observer=OBSERVER,\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "recipe = [\n",
    "    SmoothQuantModifier(smoothing_strength=0.7, ignore=ignore, mappings=mappings),\n",
    "    GPTQModifier(scheme=\"W8A8\", targets=\"Linear\", ignore=[\"lm_head\"])\n",
    "]\n",
    "\n",
    "# If the above fails, your absolute fallback is to skip the layer:\n",
    "# ignore.append(\"model.layers.31\")\n",
    "# ignore.append(\"model.layers.32\")\n",
    "# ignore.append(\"model.layers.33\")\n",
    "# ignore.append(\"model.layers.34\")\n",
    "# ignore.append(\"model.layers.35\")\n",
    "# ignore.append(\"model.layers.36\")\n",
    "# ignore.append(\"model.layers.37\")\n",
    "# ignore.append(\"model.layers.38\")\n",
    "# ignore.append(\"model.layers.39\")\n",
    "# ignore.append(\"model.layers.40\")\n",
    "# ignore.append(\"model.layers.41\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d0ba1-ca11-406b-b232-2d6edc17fcc7",
   "metadata": {},
   "source": [
    "### Quantize in One Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96817063-9eb7-4d4b-8ed9-19673f74967b",
   "metadata": {},
   "source": [
    "**How It Works**\n",
    "\n",
    "- Feeds dataset (calibration set) into your model to gather activation statistics.\n",
    "- Applies SmoothQuant rescaling followed by GPTQ quantization in a sequential per-layer manner.\n",
    "- **max_seq_length=8196** ensures large context coverage for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf36ae-0774-4cee-8f99-c367c24303d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "    max_seq_length=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105eef1b-2db4-4678-b3d5-9ebc33257424",
   "metadata": {},
   "source": [
    "### Save the Compressed Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c61fb-8493-40d5-b15c-60f18084aaab",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "- Naming: appends -W4A16 to distinguish the quantized checkpoint.\n",
    "- **save_compressed=True** stores weights in compact safetensors format for deployment via vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6d53f-8db5-40a1-874f-14d60b007b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk compressed.\n",
    "MODEL_ID = \"ibm-granite/granite-3.2-2b-instruct\"\n",
    "SAVE_DIR = MODEL_ID.split(\"/\")[-1] + \"-W4A16\"\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed216fb-9c28-4bff-8033-b68c744b4cd9",
   "metadata": {},
   "source": [
    "### Evaluate accuracy in vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02cc91-dfca-4dc1-b150-692e3a309ad3",
   "metadata": {},
   "source": [
    "We can evaluate accuracy with lm_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737002c6-1512-402f-adfa-965660478ece",
   "metadata": {},
   "source": [
    "##### Check GPU memory leftovers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cf001-f52b-46a2-a034-1a5ac99d8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2fa79-3e05-4f83-90b7-dd7be66513dc",
   "metadata": {},
   "source": [
    "**IMPORTANT**: After quantizing the model the GPU memory may not be freed (see the above output). You need to **restart the kernel** before evaluating the model to ensure you have enough GPU RAM available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5eab2-e7f1-4631-b74f-8e5b44d06dc2",
   "metadata": {},
   "source": [
    "#### Install lm_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a3ad8-effc-4212-9c5d-ab09d772e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lm_eval==v0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7facbbf2-400c-424f-bada-4f224d4f5fd0",
   "metadata": {},
   "source": [
    "#### Install vLLM for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d660bb-9435-4d1c-9599-7e9f8f1ca4ff",
   "metadata": {},
   "source": [
    "Run the following to test accuracy on GSM-8K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe802b4d-14f4-4379-a05d-e91741770c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9575f87-2dcd-4eb6-b433-9ddd56bde6f4",
   "metadata": {},
   "source": [
    "### Evaluation Command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac46967-6933-4cad-8f11-49bb61a5dd42",
   "metadata": {},
   "source": [
    "- `--model vllm` - Uses vLLM backend for fast, memory-efficient inference on large models \n",
    "- `--model_args` - pretrained=$MODEL_ID: specifies which model to load.\n",
    "- `add_bos_token=true`: ensures a beginning-of-sequence token is added; required for consistent results on math and reasoning tasks \n",
    "- `max_model_len=4096`: sets the context window the model uses for evaluation.\n",
    "- `gpu_memory_utilization=0.5`: limits vLLM to use 50% of GPU memory, allowing to avoid OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd85be-edf1-4736-86eb-fd93c09c146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "MODEL_ID = current_dir + \"/granite-3.2-2b-instruct-W4A16\"\n",
    "\n",
    "!lm_eval --model vllm \\\n",
    "  --model_args \"pretrained=$MODEL_ID,add_bos_token=true,max_model_len=4096,gpu_memory_utilization=0.5\" \\\n",
    "  --trust_remote_code \\\n",
    "  --tasks gsm8k \\\n",
    "  --num_fewshot 5 \\\n",
    "  --limit 250 \\\n",
    "  --batch_size 'auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b570baf-46e1-46e9-b970-c3f156814a69",
   "metadata": {},
   "source": [
    "With powerful GPU(s), you could also run the vLLM based evals with the following - using higher GPU memory utilization and chunked prefill. \n",
    "```bash\n",
    "!lm_eval \\\n",
    "  --model vllm \\\n",
    "  --model_args pretrained=$SAVE_DIR,dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True \\\n",
    "  --trust_remote_code \\\n",
    "  --tasks openllm \\\n",
    "  --write_out \\\n",
    "  --batch_size auto \\\n",
    "  --output_path output_dir \\\n",
    "  --show_config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a1582",
   "metadata": {},
   "source": [
    "### Upload the optimized model to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1e422-0ea3-41df-97cd-36cbcf4e5520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -q boto3\n",
    "import os\n",
    "from boto3 import client\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "OPTIMIZED_MODEL_DIR = current_dir + \"/granite-3.2-2b-instruct-W4A16\"\n",
    "S3_PATH = \"granite-int4-notebook\"\n",
    "\n",
    "print('Starting upload of quantizied model')\n",
    "s3_endpoint_url = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "s3_bucket_name = os.environ[\"AWS_S3_BUCKET\"]\n",
    "\n",
    "print(f'Uploading predictions to bucket {s3_bucket_name} '\n",
    "        f'to S3 storage at {s3_endpoint_url}')\n",
    "\n",
    "s3_client = client(\n",
    "    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,\n",
    "    aws_secret_access_key=s3_secret_key, verify=False\n",
    ")\n",
    "\n",
    "# Walk through the local folder and upload files\n",
    "for root, dirs, files in os.walk(OPTIMIZED_MODEL_DIR):\n",
    "    for file in files:\n",
    "        local_file_path = os.path.join(root, file)\n",
    "        s3_file_path = os.path.join(S3_PATH, local_file_path[len(OPTIMIZED_MODEL_DIR)+1:])\n",
    "        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)\n",
    "        print(f'Uploaded {local_file_path}')\n",
    "\n",
    "print('Finished uploading of quantizied model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa2928-327f-4e81-90be-82623d3bd8a7",
   "metadata": {},
   "source": [
    "### Bonus labs\n",
    "- Experiment with different quantization scheme & method to further improve its accuracy\n",
    "- Prepare a new dataset tailored to a specific use case by collecting and performing data mixing for calibration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
