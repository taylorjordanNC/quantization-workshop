apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: mmlu-jurisprudence-eval-job
spec:
  model: local-completions  # this should be either local-completions or local-chat-completions. If your evaluation is multiple-choice, make sure to use the former.
  taskList:
    taskNames:
      - mmlu_jurisprudence # a list of evaluation tasks to run
  logSamples: true
  batchSize: '1'
  allowOnline: true
  allowCodeExecution: false
  outputs:
    pvcManaged:
      size: 5Gi
  modelArgs:
    - name: model
      value: granite-8b # this should be the name of the InferenceService you're looking to evaluate
    - name: base_url
      value: https://granite-8b-vllm.apps.com/v1/completions # the location of your model's /chat/completions or /completions endpoint
    - name: num_concurrent
      value:  "1"
    - name: max_retries
      value:  "6"
    - name: tokenized_requests
      value: "False"
    - name: tokenizer
      value: ibm-granite/granite-3.3-8b-instruct # the tokenizer to use during the evaluation. For best results, this should match your model